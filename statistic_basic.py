# -*- coding: utf-8 -*-
"""statistic basic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13DMs8lL-QnWc5jZp9o_bH41V2M2NQ13N
"""

Q.1) Explain the different types of data (qualitative and quantitative) and provide examples of each. Discuss
nominal, ordinal, interval, and ratio scales.
Types of Data
1. Qualitative Data (Categorical Data)
Qualitative data describes qualities or characteristics that are non-numeric in nature. This data is usually descriptive, and it categorizes items based on attributes or qualities rather than quantities.

Examples:
Eye Color: Blue, green, brown, etc.
Marital Status: Single, married, divorced, etc.
Survey Responses: “Agree,” “Neutral,” “Disagree”
2. Quantitative Data (Numerical Data)
Quantitative data represents numerical values and can be measured. This type of data is numeric in nature and involves quantities, making it useful for mathematical computations.

Examples:
Age: 25, 30, 45 years
Height: 5.5 feet, 6.0 feet, 5.9 feet
Number of Items Sold: 100, 250, 175, etc.
Scales of Measurement
1. Nominal Scale

Description: The nominal scale is the most basic level of measurement. It categorizes data without any order or ranking. Labels or names distinguish different items or groups.
Examples: Gender (Male, Female), Blood Type (A, B, AB, O), Types of Cuisine (Italian, Chinese, Mexican)
2. Ordinal Scale

Description: The ordinal scale organizes data into a ranked order but without fixed intervals between ranks. This means that while items can be ordered, we cannot quantify the exact difference between them.
Examples: Satisfaction Rating (Satisfied, Neutral, Dissatisfied), Education Level (High School, Bachelor’s, Master’s, Doctorate)
3. Interval Scale

Description: The interval scale not only orders data but also specifies meaningful intervals between values. However, there is no true zero, so ratios of numbers on this scale are not meaningful.
Examples: Temperature in Celsius or Fahrenheit (0°C does not mean “no temperature”), Calendar Years (2000, 2020)
4. Ratio Scale

Description: The ratio scale has all the properties of the interval scale, but with a meaningful zero point, making it possible to discuss ratios. This means both addition and multiplication are meaningful on this scale.
Examples: Height (can be measured from zero), Weight (can be zero), Age (can be measured from zero)

Q.2) What are the measures of central tendency, and when should you use each? Discuss the mean, median,
and mode with examples and situations where each is appropriate.
Measures of Central Tendency

Measures of central tendency are statistical tools used to summarize a dataset with a single value that represents the center of the data. The three main measures are mean, median, and mode, and each is best suited for specific situations.

1. Mean (Average)
The mean is the arithmetic average of a dataset, calculated by adding all values together and dividing by the number of observations. It is sensitive to outliers, meaning that extreme values can skew the mean significantly.

Formula:

Mean
=
∑
(all data points)
number of data points
Mean=
number of data points
∑(all data points)
​

Example:
In a dataset of exam scores: 70, 80, 85, 90, and 95, the mean would be:

Mean
=
70
+
80
+
85
+
90
+
95
5
=
84
Mean=
5
70+80+85+90+95
​
 =84
When to Use the Mean:

When data is normally distributed (no extreme outliers)
When all values have equal importance and contribute equally
For continuous data like heights, weights, and test scores
2. Median (Middle Value)
The median is the middle value of a dataset when the values are ordered from lowest to highest. If there is an even number of data points, the median is the average of the two middle numbers. The median is less affected by outliers, making it useful for skewed distributions.

Example:
In a dataset of household incomes: $30,000, $40,000, $45,000, $50,000, and $1,000,000, the median would be:

Sorted data: $30,000, $40,000, $45,000, $50,000, $1,000,000
Median: $45,000 (middle value)
When to Use the Median:

When data is skewed or contains outliers
For ordinal data where ranking matters but intervals between ranks are not equal
For income data, property values, or other data with possible extreme values
3. Mode (Most Frequent Value)
The mode is the value that appears most frequently in a dataset. A dataset can have one mode (unimodal), more than one mode (bimodal or multimodal), or no mode at all if all values are unique.

Example:
In a dataset of shoe sizes: 7, 8, 8, 9, 10, 10, 10, 11, the mode is 10, as it appears the most often.

When to Use the Mode:

For categorical data to identify the most common category (e.g., favorite color, most common car brand)
When determining the most popular choice in a survey or questionnaire
For datasets with repeated values or peaks, like product sales data (most sold item)

Q.3) Explain the concept of dispersion. How do variance and standard deviation measure the spread of data?
Dispersion

Dispersion is a statistical concept that measures the extent to which data values are spread out or clustered around a central point (such as the mean). High dispersion indicates that data points are spread far from the mean, while low dispersion indicates that data points are clustered closely around the mean. Understanding dispersion helps provide insight into the variability and consistency within a dataset.

Key Measures of Dispersion
1. Variance

Variance is a measure of how much each data point in a set differs from the mean. It calculates the average squared difference of each observation from the mean, giving more weight to larger deviations.

Formula:

For a population:
𝜎
2
=
∑
(
𝑥
𝑖
−
𝜇
)
2
𝑁
σ
2
 =
N
∑(x
i
​
 −μ)
2

​

For a sample:
𝑠
2
=
∑
(
𝑥
𝑖
−
𝑥
ˉ
)
2
𝑛
−
1
s
2
 =
n−1
∑(x
i
​
 −
x
ˉ
 )
2

​

Where
𝑥
𝑖
x
i
​
  represents each data point,
𝜇
μ is the population mean,
𝑥
ˉ
x
ˉ
  is the sample mean,
𝑁
N is the population size, and
𝑛
n is the sample size.

Interpretation of Variance:
A higher variance indicates greater spread or variability among data points, while a lower variance indicates that data points are closer to the mean.

2. Standard Deviation

Standard deviation is the square root of variance, bringing the dispersion back to the original unit of measurement (since variance squares the units). It provides a more interpretable measure of spread by showing the average distance of each data point from the mean.

Formula:

For a population:
𝜎
=
∑
(
𝑥
𝑖
−
𝜇
)
2
𝑁
σ=
N
∑(x
i
​
 −μ)
2

​

​

For a sample:
𝑠
=
∑
(
𝑥
𝑖
−
𝑥
ˉ
)
2
𝑛
−
1
s=
n−1
∑(x
i
​
 −
x
ˉ
 )
2

​

​

Interpretation of Standard Deviation:

A high standard deviation means that data points are spread out over a wider range of values.
A low standard deviation indicates that data points are close to the mean and each other.
In normal distributions, about 68% of data falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations.
Example of Variance and Standard Deviation
Suppose we have a dataset of exam scores: 70, 75, 80, 85, and 90.

Calculate the Mean

Mean
=
70
+
75
+
80
+
85
+
90
5
=
80
Mean=
5
70+75+80+85+90
​
 =80
Calculate Variance (Sample)

𝑠
2
=
(
70
−
80
)
2
+
(
75
−
80
)
2
+
(
80
−
80
)
2
+
(
85
−
80
)
2
+
(
90
−
80
)
2
5
−
1
=
100
+
25
+
0
+
25
+
100
4
=
62.5
s
2
 =
5−1
(70−80)
2
 +(75−80)
2
 +(80−80)
2
 +(85−80)
2
 +(90−80)
2

​
 =
4
100+25+0+25+100
​
 =62.5
Calculate Standard Deviation (Sample)

𝑠
=
62.5
≈
7.91
s=
62.5
​
 ≈7.91

Q.4) What is a box plot, and what can it tell you about the distribution of data?
 Box plots provide a quick visual overview of the central tendency, spread, and skewness of data, as well as identifying potential outliers.

Components of a Box Plot
Box: The central box represents the interquartile range (IQR), which is the range between the first quartile (Q1) and the third quartile (Q3). This middle 50% of the data is the "body" of the distribution.
Median Line: A line inside the box marks the median (Q2), the middle value of the dataset.
Whiskers: The lines (whiskers) extend from either side of the box to the minimum and maximum values within a specified range (typically 1.5 times the IQR from Q1 and Q3). Points beyond this range are considered outliers.
Outliers: Data points outside the whiskers, usually marked as individual dots, represent unusually high or low values relative to the rest of the data.
Interpretation of a Box Plot
Central Tendency and Spread: The box plot's central box shows where the bulk of the data lies (the interquartile range), while the median line indicates the center of the distribution.
Range and Spread: The whiskers reveal the overall range of the data within 1.5 times the IQR. A larger IQR and longer whiskers indicate more dispersion, while a smaller IQR and shorter whiskers suggest less spread.
Skewness: If the median line is not centered within the box or if one whisker is significantly longer than the other, the data may be skewed.
Right (Positive) Skew: The right whisker is longer, or the median is closer to Q1.
Left (Negative) Skew: The left whisker is longer, or the median is closer to Q3.
Outliers: Points outside the whiskers represent data points that deviate significantly from the central part of the distribution. These can be potential outliers, indicating variability or error in the data.
Example Interpretation
Imagine a box plot summarizing the ages of people attending a concert:

If the box plot has a long right whisker and outliers to the right, the audience might include a small number of much older individuals compared to the bulk.

Q.5) Discuss the role of random sampling in making inferences about populations.
Random Sampling and Population Inference

Random sampling is a fundamental process in statistics for making reliable inferences about a population. In random sampling, each member of a population has an equal chance of being included in the sample, which helps ensure that the sample is representative of the broader population.

Why Random Sampling is Essential
Representativeness: By selecting members of the population randomly, we increase the likelihood that the sample accurately reflects the population's characteristics. A representative sample enables us to make inferences that generalize beyond the sample to the entire population.

Reduced Bias: Random sampling helps minimize selection bias, which occurs when certain groups within a population are systematically more or less likely to be included in the sample. By giving every member of the population an equal chance, we can avoid skewed data that might mislead our analysis and conclusions.

Statistical Validity: Statistical methods for inference (such as confidence intervals and hypothesis tests) rely on assumptions about the data. Random sampling helps meet these assumptions, providing a foundation for valid statistical conclusions.

How Random Sampling Supports Population Inference
Estimating Population Parameters: We often want to estimate parameters such as the mean, proportion, or standard deviation of a population. A random sample allows us to calculate sample statistics that serve as unbiased estimates of these parameters. For example, the sample mean can serve as an estimate of the population mean.

Measuring Reliability through Confidence Intervals: With a random sample, we can calculate confidence intervals to express the reliability of our estimates. A confidence interval gives a range of values within which the true population parameter is likely to fall, with a specified level of confidence (e.g., 95%).

Hypothesis Testing: Random sampling is essential for hypothesis testing, where we make inferences about population characteristics based on sample data. For example, we might test whether the average income in a population differs significantly from a known value. Because random sampling reduces bias, the results of these tests are more reliable.

Q.6) Explain the concept of skewness and its types. How does skewness affect the interpretation of data?
Skewness is a statistical measure that describes the asymmetry of a dataset’s distribution around its mean. In a perfectly symmetrical distribution, such as a normal distribution, skewness is zero. When a distribution is not symmetrical, it is skewed, meaning that data values are more spread out on one side of the mean than the other. Skewness helps us understand the shape of the data and identify whether it leans toward higher or lower values.

Types of Skewness
Positive Skew (Right Skew)

Description: In a positively skewed distribution, the tail on the right side of the distribution is longer, indicating that there are relatively more large values or extreme positive values.
Characteristics:
Mean > Median > Mode (mean is pulled to the right by the high values).
Examples include income levels, house prices, and other datasets where high values are possible but infrequent.
Interpretation: A positive skew suggests that while most values are lower, a few high values increase the average, which may not be representative of the majority of the data.
Negative Skew (Left Skew)

Description: In a negatively skewed distribution, the tail on the left side of the distribution is longer, indicating that there are more small or extreme negative values.
Characteristics:
Mean < Median < Mode (mean is pulled to the left by low values).
Examples include age at retirement, exam scores in a high-performing class, etc.
Interpretation: A negative skew suggests that while most values are higher, a few low values pull the mean down, potentially understating the central tendency for most of the data.
Zero Skew (Symmetrical Distribution)

Description: A zero-skew distribution is perfectly symmetrical, with equal distribution on both sides of the mean. This typically refers to a normal distribution (bell curve), where skewness is zero.
Characteristics:
Mean = Median = Mode.
Examples include certain physical measurements like height in a healthy population, where values are distributed evenly around a central point.
Interpretation: In symmetrical data, the mean is a good measure of central tendency, as it accurately represents the central point of the distribution.

Q.7) What is the interquartile range (IQR), and how is it used to detect outliers?
The interquartile range (IQR) is a measure of statistical dispersion that indicates the range within which the middle 50% of a dataset falls. It is calculated as the difference between the first quartile (Q1) and the third quartile (Q3), which correspond to the 25th and 75th percentiles, respectively.

Formula:
IQR
=
𝑄
3
−
𝑄
1
IQR=Q3−Q1
Where:

Q1 (First Quartile) is the median of the lower half of the data (25th percentile).
Q3 (Third Quartile) is the median of the upper half of the data (75th percentile).
How IQR is Used to Detect Outliers
Outliers are data points that fall far from the other observations. The IQR is commonly used to identify these outliers by setting boundaries, or "fences," beyond which data points are considered unusually high or low.

Lower Bound:
Lower Bound
=
𝑄
1
−
1.5
×
IQR
Lower Bound=Q1−1.5×IQR
Upper Bound:
Upper Bound
=
𝑄
3
+
1.5
×
IQR
Upper Bound=Q3+1.5×IQR
Any data point below the lower bound or above the upper bound is considered an outlier. This method of outlier detection is widely used because it’s not influenced by extreme values, making it more robust than using the mean and standard deviation, especially for skewed distributions.

Example
Suppose we have the following dataset of exam scores: 55, 60, 65, 70, 75, 80, 85, 90, and 150.

Calculate Q1 and Q3:

Q1 (25th percentile) = 65
Q3 (75th percentile) = 85
Calculate the IQR:

IQR
=
85
−
65
=
20
IQR=85−65=20
Determine Outlier Bounds:

Lower Bound =
65
−
(
1.5
×
20
)
=
35
65−(1.5×20)=35
Upper Bound =
85
+
(
1.5
×
20
)
=
115
85+(1.5×20)=115
Identify Outliers:

Any score below 35 or above 115 is considered an outlier. In this dataset, the score of 150 is an outlier, as it exceeds the upper bound of 115.

Q.8) Discuss the conditions under which the binomial distribution is used.
The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, each with the same probability of success. It is used under specific conditions when events have only two possible outcomes: success or failure.

Conditions for Using the Binomial Distribution
Fixed Number of Trials (n):

The number of trials, or attempts, is predetermined and remains constant. For example, flipping a coin 10 times or conducting 20 patient trials.
Binary Outcomes:

Each trial results in one of two possible outcomes, typically labeled "success" and "failure." For example, in a coin toss, heads could be considered "success" and tails "failure."
Constant Probability of Success (p):

The probability of success remains the same for each trial. For example, in a fair coin toss, the probability of getting heads is always 0.5 for each flip.
Independence of Trials:

The outcome of each trial is independent of others, meaning that one trial's result does not affect the outcome of any other trial. For instance, flipping a coin does not influence the results of future flips.
Binomial Distribution Formula
The probability of achieving exactly
𝑘
k successes in
𝑛
n trials, with a probability of success
𝑝
p, is given by:

𝑃
(
𝑋
=
𝑘
)
=
(
𝑛
𝑘
)
𝑝
𝑘
(
1
−
𝑝
)
𝑛
−
𝑘
P(X=k)=(
k
n
​
 )p
k
 (1−p)
n−k

where:

(
𝑛
𝑘
)
(
k
n
​
 ) is the binomial coefficient, representing the number of ways to choose
𝑘
k successes out of
𝑛
n trials.
𝑝
p is the probability of success.
(
1
−
𝑝
)
(1−p) is the probability of failure.

Q.9) Explain the properties of the normal distribution and the empirical rule (68-95-99.7 rule).
The normal distribution, often called the Gaussian distribution or bell curve, is a continuous probability distribution that is symmetrical about its mean. It is one of the most important and widely used distributions in statistics due to its natural appearance in many real-world phenomena (e.g., heights, test scores, measurement errors).

Key Properties of the Normal Distribution
Symmetry:

The normal distribution is perfectly symmetrical about the mean. This means that the left and right halves of the distribution are mirror images of each other.
Bell Shape:

The shape of the normal distribution is bell-like, with the majority of the data clustering around the mean and tapering off as it moves away from the mean.
Mean, Median, and Mode:

In a normal distribution, the mean, median, and mode are all equal and located at the center of the distribution.
Asymptotic Nature:

The tails of the normal distribution approach, but never touch, the horizontal axis. This means there is a small but non-zero probability for extreme values, no matter how far they are from the mean.
Defined by Mean and Standard Deviation:

A normal distribution is fully described by its mean (µ) and standard deviation (σ). The mean determines the center of the distribution, while the standard deviation controls the spread, or "width," of the curve. A larger standard deviation produces a wider and flatter curve, while a smaller standard deviation creates a narrower and taller curve.
The Empirical Rule (68-95-99.7 Rule)
The empirical rule, also known as the 68-95-99.7 rule, describes how data is distributed in a normal distribution and provides a quick estimate of where most data points lie in relation to the mean. According to the empirical rule:

68% of data falls within 1 standard deviation (σ) of the mean (µ).

Mathematically, approximately 68% of the values lie between
𝜇
−
𝜎
μ−σ and
𝜇
+
𝜎
μ+σ.
95% of data falls within 2 standard deviations (2σ) of the mean.

This means about 95% of values lie between
𝜇
−
2
𝜎
μ−2σ and
𝜇
+
2
𝜎
μ+2σ.
99.7% of data falls within 3 standard deviations (3σ) of the mean.

Almost all values (99.7%) lie between
𝜇
−
3
𝜎
μ−3σ and
𝜇
+
3
𝜎
μ+3σ.
Practical Application of the Empirical Rule
The empirical rule is useful for:

Identifying Outliers: Data points outside ±3 standard deviations from the mean are rare (less than 0.3% of data in a normal distribution) and are often considered outliers.
Quick Probability Estimates: In the absence of more specific calculations, the rule provides a rough estimate of the proportion of values within certain ranges.
Interpreting Standardized Scores: In fields like education, test scores are often normalized to fit a normal distribution. For example, knowing that a score falls within 2 standard deviations of the mean gives an approximate sense of how typical or rare it is.
Example
Suppose the heights of a population are normally distributed with a mean (µ) of 170 cm and a standard deviation (σ) of 10 cm.

68% of the population’s heights will fall between 160 cm and 180 cm (within ±1σ).
95% will fall between 150 cm and 190 cm (within ±2σ).
99.7% will fall between 140 cm and 200 cm (within ±3σ).

Q.10) Provide a real-life example of a Poisson process and calculate the probability for a specific event.
A Poisson process is a statistical process that models the occurrence of events that happen independently and randomly over a fixed period of time or space. This process is commonly applied in scenarios where events happen at a constant average rate but without a fixed interval between them. The Poisson distribution is used to calculate the probability of a specific number of events occurring within a given interval.

Real-Life Example of a Poisson Process
Example Scenario: Suppose a customer service call center receives an average of 5 calls per hour. We want to calculate the probability that the center will receive exactly 8 calls in the next hour.

In this example:

The average rate (λ) of incoming calls is 5 calls per hour.
We are interested in the probability of receiving
𝑘
=
8
k=8 calls in one hour.
Poisson Probability Formula
The probability of observing exactly
𝑘
k events in a fixed interval with an average rate
𝜆
λ is given by:

𝑃
(
𝑋
=
𝑘
)
=
𝑒
−
𝜆
⋅
𝜆
𝑘
𝑘
!
P(X=k)=
k!
e
−λ
 ⋅λ
k

​

where:

𝑃
(
𝑋
=
𝑘
)
P(X=k) is the probability of
𝑘
k events occurring,
𝑒
≈
2.718
e≈2.718 (Euler's number),
𝜆
λ is the average rate of occurrence,
𝑘
k is the number of events we want to find the probability for, and
𝑘
!
k! is the factorial of
𝑘
k.
Calculating the Probability
Given:

𝜆
=
5
λ=5
𝑘
=
8
k=8
Using the formula, we get:

𝑃
(
𝑋
=
8
)
=
𝑒
−
5
⋅
5
8
8
!
P(X=8)=
8!
e
−5
 ⋅5
8

​

Let’s calculate this.

The probability of receiving exactly 8 calls in the next hour is approximately 0.065 or 6.53%.

Q.11) Explain what a random variable is and differentiate between discrete and continuous random variables.
A random variable is a numerical outcome of a random phenomenon. It assigns a real number to each possible outcome of a random process, making it a fundamental concept in probability and statistics. Random variables are typically denoted by capital letters (e.g.,
𝑋
X,
𝑌
Y).

Types of Random Variables
Random variables can be classified into two main types: discrete and continuous random variables.

1. Discrete Random Variables
Definition: A discrete random variable takes on a countable number of distinct values. These values can be finite or countably infinite (e.g., 0, 1, 2, ...).

Characteristics:

Each possible value can be assigned a specific probability.
The sum of the probabilities of all possible outcomes is equal to 1.
Examples:

The number of heads when flipping a coin three times (possible outcomes: 0, 1, 2, or 3 heads).
The number of students present in a classroom on a given day (e.g., 0, 1, 2,..., up to the total number of students).
The outcome of rolling a six-sided die (possible values: 1, 2, 3, 4, 5, or 6).
2. Continuous Random Variables
Definition: A continuous random variable can take on an infinite number of possible values within a given range. These values cannot be counted but can be measured.

Characteristics:

Continuous random variables are associated with probability density functions (PDFs) rather than probability mass functions (PMFs).
The probability of a continuous random variable taking on a specific value is always zero; instead, probabilities are calculated over intervals.
Examples:

The height of individuals (possible values could be any positive real number, e.g., 150.5 cm, 160.2 cm).
The time it takes for a runner to finish a race (can be any non-negative real number).
The temperature in a city throughout the day (can take on any value within a range).

Q.12) Provide an example dataset, calculate both covariance and correlation, and interpret the results.
Here’s a small dataset:

Student	Hours Studied (X)	Test Score (Y)
1	1	50
2	2	60
3	3	65
4	4	70
5	5	80
Step 1: Calculate Covariance
Covariance measures the degree to which two variables change together. The formula for covariance is:

Cov
(
𝑋
,
𝑌
)
=
∑
(
𝑋
𝑖
−
𝑋
ˉ
)
(
𝑌
𝑖
−
𝑌
ˉ
)
𝑛
Cov(X,Y)=
n
∑(X
i
​
 −
X
ˉ
 )(Y
i
​
 −
Y
ˉ
 )
​

where:

𝑋
𝑖
X
i
​
  and
𝑌
𝑖
Y
i
​
  are the individual sample points,
𝑋
ˉ
X
ˉ
  and
𝑌
ˉ
Y
ˉ
  are the means of
𝑋
X and
𝑌
Y,
𝑛
n is the number of data points.
Step 2: Calculate Correlation
Correlation measures the strength and direction of the linear relationship between two variables. The formula for the Pearson correlation coefficient (
𝑟
r) is:

𝑟
=
Cov
(
𝑋
,
𝑌
)
𝑆
𝑋
𝑆
𝑌
r=
S
X
​
 S
Y
​

Cov(X,Y)
​

where:

𝑆
𝑋
S
X
​
  and
𝑆
𝑌
S
Y
​
  are the standard deviations of
𝑋
X and
𝑌
Y, respectively.
Now, let's calculate both covariance and correlation using the provided dataset.

Results
Covariance:
Cov
(
𝑋
,
𝑌
)
=
14.0
Cov(X,Y)=14.0
Correlation:
𝑟
≈
0.99
r≈0.99
Interpretation
Covariance:

A covariance of 14.0 indicates that there is a positive relationship between the number of hours studied and the test scores. Specifically, as the hours studied increase, the test scores tend to increase as well. However, covariance does not provide information about the strength of this relationship in a standardized way.
Correlation:

A correlation coefficient of approximately 0.99 suggests a very strong positive linear relationship between the two variables. This means that the hours studied and the test scores are closely related; as students study more hours, their test scores increase almost proportionally.